\documentclass[11pt]{elegantbook}

\title{Introduction to Probability Theory}
\subtitle{Quick reference book created from Math 531 Lecture Notes}

\author{Chris Cai/ Linrong Cai}
\institute{University of Wisconisn Madison}
\date{May 15, 2023}
\version{1.0}
\bioinfo{Instructor}{Benedek Valkó}

\extrainfo{The theory of probabilities is at bottom nothing but common sense reduced to calculus; it enables us to appreciate with exactness that which accurate minds feel with a sort of instinct for which of times they are unable to account.}
\vspace{30mm}
\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Events and Probability}

\section{Kolmogorov's axioms and probability space}

These are Kolmogorov's axioms for probability theory.
\begin{definition}
A $\textbf{probability space}$ is a triple $(\Omega, \mathcal{F}, P)$, with the following components. \\
(a) $\Omega$ is a set, called the $\textbf{sample space}$. \\ 
(b) $\mathcal{F}$ is a collection of subsets of $\Omega$. Members of $\mathcal{F}$ are called events. $\mathcal{F}$ is assumed to be a $\sigma$-algebra, which means that it satisfies the following three properties.\\
(b.1) $\Omega \in \mathcal{F}$. That is, the whole sample space itself is an event.\\
(b.2) If $A \in \mathcal{F}$ then $A^c \in \mathcal{F}$.\\
(b.3) If $\left\{A_k\right\}_{1 \leq k<\infty}$ is a sequence of members of $\mathcal{F}$, then their union $\bigcup_{k=1}^{\infty} A_k$ is also a member of $\mathcal{F}$.\\
(c) $P$ is a function from $\mathcal{F}$ into real numbers, called the $\textbf{probability measure}$ . $P$ satisfies the following axioms.\\
(c.1) $0 \leq P(A) \leq 1$ for each event $A \in \mathcal{F}$.\\
(c.2) $P(\varnothing)=0$ and $P(\Omega)=1$.\\
(c.3) If $\left\{A_k\right\}_{1 \leq k<\infty}$ is a sequence of pairwise disjoint events then
$$
P\left(\bigcup_{k=1}^{\infty} A_k\right)=\sum_{k=1}^{\infty} P\left(A_k\right) .
$$
\end{definition}

\begin{note}
    In mathematical analysis and in probability theory, a $\boldsymbol{\sigma}$-algebra (also $\boldsymbol{\sigma}$-field) on a set $X$ is a nonempty collection $\Sigma$ of subsets of $X$ closed under complement, countable unions, and countable intersections. The ordered pair $(X, \Sigma)$ is called a measurable space.
\end{note}


\section{Inclusion Exclusion Principle}

\begin{theorem}[inclusion-exclusion principle]
Let $A_1, A_2, A_3, \ldots$ be events in some probability space $(\Omega, \mathcal{F}, P)$. Then for each integer $n \geq 2$,
$$
\begin{aligned}
P\left(A_1 \cup \cdots \cup A_n\right)= & \sum_{i=1}^n P\left(A_i\right)-\sum_{1 \leq i_1<i_2 \leq n} P\left(A_{i_1} \cap A_{i_2}\right) \\
& +\sum_{1 \leq i_1<i_2<i_3 \leq n} P\left(A_{i_1} \cap A_{i_2} \cap A_{i_3}\right) \\
& -\sum_{1 \leq i_1<i_2<i_3<i_4 \leq n} P\left(A_{i_1} \cap A_{i_2} \cap A_{i_3} \cap A_{i_4}\right) \\
& +\cdots+(-1)^{n+1} P\left(A_1 \cap \cdots \cap A_n\right) . \\
= & \sum_{k=1}^n(-1)^{k+1} \sum_{1 \leq i_1<\cdots<i_k \leq n} P\left(A_{i_1} \cap \cdots \cap A_{i_k}\right) .
\end{aligned}
$$
This is called the inclusion-exclusion identity.
\end{theorem}

\section{Monotonicity and Countable Subadditivity}

\begin{proposition}
Let $A, B, A_1, A_2, A_3, \ldots$ be events in some probability space $(\Omega, \mathcal{F}, P)$\\
(i) Monotonicity: if $A \subset B$ then $P(A) \leq P(B)$.\\
(ii) Countable subadditivity: for any sequence of events $\left\{A_k\right\}$,
$$
P\left(\bigcup_{k=1}^{\infty} A_k\right) \leq \sum_{k=1}^{\infty} P\left(A_k\right) .
$$
Countable subadditivity generalizes the countable additivity axiom in a natural way. Its truth should be fairly obvious because the union $\bigcup_{k=1}^{\infty} A_k$ can have overlaps whose probabilities are then counted several times over in the sum $\sum_{i=1}^{\infty} P\left(A_k\right)$. By taking $A_k=\varnothing$ for all $k>n$ we get a finite version of subadditivity:
$$
P\left(A_1 \cup \cdots \cup A_n\right) \leq P\left(A_1\right)+\cdots+P\left(A_n\right)
$$
valid for all events $A_1, \ldots, A_n$.
\end{proposition}

\begin{corollary}
Let $\left\{A_k\right\}$ be a sequence of events on $(\Omega, \mathcal{F}, P)$.\\
\; (i) If $P\left(A_k\right)=0$ for all $k$, then $P\left(\bigcup_k A_k\right)=0$.\\
\; (ii) If $P\left(A_k\right)=1$ for all $k$, then $P\left(\bigcap_k A_k\right)=1$.
\end{corollary}

\section{Continuity of Probability}
\begin{definition}
    Suppose $\left\{A_k\right\}_{k \in \mathbb{Z}_{>0}},\left\{B_k\right\}_{k \in \mathbb{Z}_{>0}}, A$, and $B$ are events in a probability space
    $(\Omega, \mathcal{F}, P)$. We say that $A_k$ increases up to $A$ and use the notation
$$
A_k \nearrow A
$$
if the events $A_k$ are nested nondecreasing, which means that $A_1 \subset A_2 \subset A_3 \subset$ $\cdots \subset A_k \subset \cdots$, and $A=\bigcup_k A_k$. Figure 1 illustrates.
Analogously, we say that $B_k$ decreases down to $B$ and use the notation
$$
B_k \searrow B
$$
if the events $B_k$ are nested nonincreasing, which means that $B_1 \supset B_2 \supset B_3 \supset \cdots \supset$ $B_k \supset \cdots$, and $B=\bigcap_k B_k$
\end{definition}
\begin{theorem}

If $A_k \nearrow A$ or $A_k \searrow A$, then the probabilities converge: $\lim _{k \rightarrow \infty} P\left(A_k\right)=$ $P(A)$
\end{theorem}

\section{Conditional Probability}
\begin{definition}{conditional probability}
Let $B$ be an event on the probability space $(\Omega, \mathcal{F}, P)$ such that $P(B)>0$. Then for all events $A \in \mathcal{F}$ the conditional probability of $A$ given $B$ is defined as
$$
P(A \mid B)=\frac{P(A B)}{P(B)}
$$
\end{definition}

\begin{proposition}
  Let $B$ be an event on the probability space $(\Omega, \mathcal{F}, P)$ such that $P(B)>0$. Then as a function of the event $A, P(A \mid B)$ is a probability measure on $(\Omega, \mathcal{F})$
\end{proposition}

\begin{theorem}
In each statement below all events are on the same probability $\operatorname{space}(\Omega, \mathcal{F}, P)$\\
(a) Let $A$ and $B$ be two events and assume $P(B)>0$. Then
$$
P(A B)=P(B) P(A \mid B)
$$
Let $A_1, \ldots, A_n$ be events and assume $P\left(A_1 \cdots A_{n-1}\right)>0$. Then
$$
P\left(A_1 A_2 \cdots A_n\right)=P\left(A_1\right) P\left(A_2 \mid A_1\right) P\left(A_3 \mid A_1 A_2\right) \cdots P\left(A_n \mid A_1 \cdots A_{n-1}\right)
$$
(b) Let $\left\{B_i\right\}$ be a countable partition of $\Omega$ and $A$ any event. Then
$$
P(A)=\sum_{i: P\left(B_i\right)>0} P\left(A \mid B_i\right) P\left(B_i\right) .
$$
The sum above ranges over those indices $i$ such that $P\left(B_i\right)>0$.
\end{theorem}

\section{Bayes' Formula}

\begin{theorem}{Bayes' formula}
Let $\left\{B_k\right\}$ be a countable partition of the sample space $\Omega$. Then for any event $A$ with $P(A)>0$ and each $k$ such that $P\left(B_k\right)>0$,
$$
P\left(B_k \mid A\right)=\frac{P\left(A B_k\right)}{P(A)}=\frac{P\left(A \mid B_k\right) P\left(B_k\right)}{\sum_{i: P\left(B_i\right)>0} P\left(A \mid B_i\right) P\left(B_i\right)}
$$
\end{theorem}

\section{Independent Events}

\begin{definition}
    Two events $A$ and $B$ are independent if
$$
P(A B)=P(A) P(B)
$$
\end{definition}

\begin{theorem}
Suppose that $A$ and $B$ are independent events. Then the same is true for each of these pairs: $A^c$ and $B, A$ and $B^c$, and $A^c$ and $B^c$.
\end{theorem}

The definition of independence of more than two events requires that the product property hold for any subcollection of events.

\begin{definition}
(a) Events $A_1, \ldots, A_n$ are independent (or mutually independent) if for every collection $A_{i_1}, \ldots, A_{i_k}$, where $2 \leq k \leq n$ and $1 \leq i_1<i_2<\cdots<$ $i_k \leq n$
$$
P\left(A_{i_1} A_{i_2} \cdots A_{i_k}\right)=P\left(A_{i_1}\right) P\left(A_{i_2}\right) \cdots P\left(A_{i_k}\right)
$$\\
(b) Let $\left\{A_k\right\}_{k \in \mathbb{Z}_{>0}}$ be an infinite sequence of events in a probability space $(\Omega, \mathcal{F}, P)$. Then events $\left\{A_k\right\}_{k \in \mathbb{Z}_{>0}}$ are independent if for each $n \in \mathbb{Z}_{>0}$, events $A_1, \ldots, A_n$ are independent.
\end{definition}

\begin{theorem}
(a) Suppose events $A_1, \ldots, A_n$ are mutually independent. Then for every collection $A_{i_1}, \ldots, A_{i_k}$, where $2 \leq k \leq n$ and $1 \leq i_1<i_2<\cdots<i_k \leq n$, we have
$$
P\left(A_{i_1}^* A_{i_2}^* \cdots A_{i_k}^*\right)=P\left(A_{i_1}^*\right) P\left(A_{i_2}^*\right) \cdots P\left(A_{i_k}^*\right)
$$
where each $A_i^*$ can represent either $A_i$ or $A_i^c$.\\
(b) Let $\left\{A_k\right\}_{k \geq 1}$ be a finite or infinite sequence of independent events. Let $0=k_0<k_1<\cdots<k_n$ be integers. Let $B_1, \ldots, B_n$ be events constructed from the $A_k s$ so that, for each $j=1, \ldots, n, B_j$ is formed by applying set operations to $A_{k_{j-1}+1}, \ldots, A_{k_j}$. Then the events $B_1, \ldots, B_n$ are independent.
\end{theorem}

\begin{definition}
Let $A_1, \ldots, A_n$ and $B$ be events on $(\Omega, \mathcal{F}, P)$ and assume $P(B)>$ 0 . Then events $A_1, \ldots, A_n$ are conditionally independent, given $B$, if for every collection $A_{i_1}, \ldots, A_{i_k}$, where $2 \leq k \leq n$ and $1 \leq i_1<i_2<\cdots<i_k \leq n$,
$$
P\left(A_{i_1} A_{i_2} \cdots A_{i_k} \mid B\right)=\prod_{j=1}^k P\left(A_{i_j} \mid B\right)
$$
\end{definition}

\chapter{Random Variables and Probability Distributions}
\section{Random Variables}
Often we are interested in some numerical value associated to the outcome of
a random experiment. This just means that we are interested in the value of a
function that maps the elements of the sample space into the real numbers. These
functions are called random variables.

\begin{definition}
Let $(\Omega, \mathcal{F}, P)$ be a probability space. A random variable on $\Omega$ is a real valued function $X: \Omega \rightarrow \mathbb{R}$, for which $\{\omega \in \Omega: X(\omega) \leq c\} \in \mathcal{F}$ for any $c \in \mathbb{R}$.
\end{definition}

There is a simple way to encode an events as a random variable.

\begin{definition}
 Let $B$ be an event in a probability space. Then the indicator function (or indicator random variable) of $B$ is defined as the function
$$
I_B(\omega)= \begin{cases}1, & \text { if } \omega \in B \\ 0, & \text { if } \omega \notin B\end{cases}
$$
\end{definition}

\begin{note}
Note that $I_B$ is a random variable.
\end{note}
\section{Probability Distributions}

Through the probabilities of events of type $\{X \in B\}$, a random variable induces a probability measure on the real line.

\begin{definition}
Let $X$ be a random variable defined on the probability space $(\Omega, \mathcal{F}, P)$. The probability distribution of $X$ is the probability measure $\mu$ on $\mathbb{R}$ defined by
$$
\mu(B)=P(X \in B)
$$
for Borel subsets $B$ of $\mathbb{R}$.
\end{definition}

\begin{note}
    In mathematics, a Borel set is any set in a topological space that can be formed from open sets (or, equivalently, from closed sets) through the operations of countable union, countable intersection, and relative complement. Borel sets are named after Émile Borel.
\end{note}

\section{Discrete Random Variables}

\begin{definition}
A random variable $X$ is discrete if there exists a finite or countably infinite set $B \subset \mathbb{R}$ such that $P(X \in B)=1$.\\

We say that those values $k$ for which $P(X=k)>0$ are the possible values of the discrete random variable $X$.

As for functions in general, the range of a random variable $X$ is the set of all its values: the range of $X$ is the set $\{X(\omega): \omega \in \Omega\}$. In particular, if the range of
the random variable $X$ is finite or countably infinite, then $X$ is a discrete random variable.
\end{definition}

\section{Probability Mass Functions}
\begin{definition}
The probability mass function (p.m.f.) of a discrete random variable $X$ is the function $p\left(\right.$ or $\left.p_X\right)$ defined by
$$
p(k)=P(X=k)
$$
for the possible values $k$ of $X$. (In some cases it is convenient to extend the function $p$ for other values as well, we can use the same definition even if $P(X=k)=0$.) \\
Probabilities of events involving $X$ come by summing values of the probability mass function: for any subset $B \subseteq \mathbb{R}$
$$
P(X \in B)=\sum_{k \in B} P(X=k)=\sum_{k \in B} p_X(k)
$$
where the sum is over the possible values $k$ of $X$ that lie in $B$.
\end{definition}

\section{Bernoulli Distribution}
\begin{definition}
Let $0 \leq p \leq 1$. We say that a random variable $X$ has Bernoulli distribution with parameter $p$ if $X$ is a discrete random variable with probability mass function
$$
p_X(1)=p, \quad p_X(0)=1-p
$$
We abbreviate this as $X \sim \operatorname{Ber}(p)$.
\end{definition}
\begin{note}
The distribution of an indicator random variable $I_B$ is always Bernoulli, its parameter is $P(B)$.
\end{note}
\section{Cumulative distribution functions}
\begin{definition}
Let $X$ be a random variable defined on the probability space $(\Omega, \mathcal{F}, P)$. The cumulative distribution function (c.d.f.) of $X$ is defined by
$F(s)=P(X \leq s) \quad$ for all $s \in \mathbb{R}$.
\end{definition}
\begin{proposition}
    (a) Let $F: \mathbb{R} \rightarrow[0,1]$ be the cumulative distribution function of a random variable $X$. Then $F$ has the following properties.\\
(i) Monotonicity: if $s<t$ then $F(s) \leq F(t)$.\\
(ii) Right continuity: $F(t)=F(t+)$ for each $t \in \mathbb{R}$.\\
(iii) $\lim _{t \rightarrow-\infty} F(t)=0$ and $\lim _{t \rightarrow \infty} F(t)=1$.\\
(b) Conversely, if a function $F: \mathbb{R} \rightarrow[0,1]$ has properties (i)-(iii) above, then $F$ is the cumulative distribution function of some random variable.\\
(c) Let $X$ be a random variable with cumulative distribution function $F$. Then for any $s \in \mathbb{R}$ we have these identities:
$$
P(X<s)=F(s-)
$$
and
$$
P(X=s)=F(s)-F(s-) .
$$
\end{proposition}

\section{Probability Density Function}

\begin{definition}
Let $X$ be a random variable on $(\Omega, \mathcal{F}, P)$. If a function $f$ on $\mathbb{R}$ satisfies $f(x) \geq 0$ for all $x$ and
$$
P(X \leq b)=\int_{-\infty}^b f(x) d x
$$
for all real values $b$, then $f$ is the probability density function (p.d.f.) of $X$. When $X$ has a density function, we call $X$ an absolutely continuous random variable.
\end{definition}

\begin{theorem}
 Suppose the cumulative distribution function $F$ of the random variable $X$ is continuous and the derivative $F^{\prime}(x)$ exists everywhere on the real line, except possibly at countably many points. Then $X$ is an absolutely continuous random variable and $f(x)=F^{\prime}(x)$ is the density function of $X$. If $F$ is not differentiable at a point $x$, then the value $f(x)$ can be set arbitrarily.
\end{theorem}

\begin{definition}
Let $f$ be a piecewise continuous function on $\mathbb{R}$. Then $f$ is the density function of a random variable if and only if
$f(x) \geq 0$ for all $x \in \mathbb{R}$ and $\int_{-\infty}^{\infty} f(x) d x=1$
\end{definition}
\begin{corollary}
    Random variables can be neither discrete nor absolutely continuous.
\end{corollary}
\begin{example}
 Fix $a<b$ and define $F: \mathbb{R} \rightarrow[0,1]$ by
$$
F(x)= \begin{cases}0 & \text { if } x<a \\ \frac{1}{3} \cdot \frac{x-a}{b-a} & \text { if } a \leq x<b \\ 1 & \text { if } x \geq b .\end{cases}
$$
\end{example}
\begin{note}
    It is natural to generalize the above to random vectors. 
\end{note}
\section{Uniform Distribution}

\begin{definition}{Uniform distribution on an interval.}
Let $[a, b]$ be a bounded interval on the real line. Random variable $X$ has the uniform distribution on the interval $[a, b]$ if $X$ has density function
$$
f(x)= \begin{cases}\frac{1}{b-a}, & \text { if } x \in[a, b] \\ 0, & \text { if } x \notin[a, b]\end{cases}
$$
Abbreviate this by $X \sim \operatorname{Unif}[a, b]$
If $X \sim \operatorname{Unif}[a, b]$ and $[c, d] \subset[a, b]$, then
$$
P(c \leq X \leq d)=\int_c^d \frac{1}{b-a} d x=\frac{d-c}{b-a}
$$
\end{definition}

\begin{definition}
 Let $\Omega$ be a subset of $d$-dimensional Euclidean space $\mathbb{R}^d$ with finite volume. Then the random point $\mathbf{X}$ is uniformly distributed on $\Omega$ if its joint density function is
$$
f(\mathbf{x})= \begin{cases}\frac{1}{\operatorname{vol}(\Omega)} & \text { if } \mathbf{x} \in \Omega \\ 0 & \text { if } \mathbf{x} \notin \Omega\end{cases}
$$
\end{definition}

\section{Notion of equality}

\begin{definition}{ Almost sure equality}
   
Let $X$ and $Y$ be random variables defined on $(\Omega, \mathcal{F}, P)$. Then $X$ and $Y$ are equal almost surely if $P(X=Y)=1$. This is abbreviated by $X=Y$ a.s.


\end{definition}

\begin{note}
    Almost sure equality is also expressed by saying $X=Y$ with probability one, abbreviated $X=Y$ w.p.1. Below is a discrete and an absolutely continuous example of almost sure equality $X=Y$ where pointwise equality fails.
\end{note}
\begin{example}
    Let $\Omega=\{1,2,3\}$ with probability measure $P\{1\}=P\{2\}=\frac{1}{2}$ and $P\{3\}=0$. Define random variables $X$ and $Y$ on $\Omega$ by
$$
X(1)=Y(1)=1, X(2)=Y(2)=2, X(3)=3 \text { and } Y(3)=0 .
$$
Then $P(X=Y)=P\{1,2\}=1$.
\end{example}

\begin{definition}{Equality in distribution}
Random variables $X$ and $Y$ are equal in distribution if $P(X \in$ $B)=P(Y \in B)$ for all Borel subsets $B$ of $\mathbb{R}$. This is abbreviated by $X \stackrel{d}{=} Y$. 
\end{definition}

\begin{theorem}
Suppose $X$ and $Y$ are random variables on the same probability space $(\Omega, \mathcal{F}, P)$. Then $P(X=Y)=1$ implies $X \stackrel{d}{=} Y$.
\end{theorem}

\chapter{Independent and dependent random variables}
\section{Independence}
\begin{definition}
(a) Let $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ be random vectors defined on the same probability space. Let $\mathbf{X}_i$ be $\mathbb{R}^{d_i}$-valued. Then $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ are independent if
$$
P\left(\mathbf{X}_1 \in B_1, \mathbf{X}_2 \in B_2, \ldots, \mathbf{X}_n \in B_n\right)=\prod_{k=1}^n P\left(\mathbf{X}_k \in B_k\right)
$$
for all Borel subsets $B_i \subset \mathbb{R}^{d_i}, i=1, \ldots, n$.\\\\
(b) Let $\left\{\mathbf{X}_k\right\}_{k \in \mathbb{Z}_{>0}}$ be an infinite sequence of random vectors defined on some probability space $(\Omega, \mathcal{F}, P)$. Then the random vectors $\left\{\mathbf{X}_k\right\}_{k \in \mathbb{Z}_{>0}}$ are independent if, for each $n \in \mathbb{Z}_{>0}$, the random vectors $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ are independent.

\end{definition}

\begin{theorem}
 Let $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ be random vectors defined on the same probability space. For $1 \leq i \leq n$ let $d_i$ be the dimension of $\mathbf{X}_i$ and set $d=d_1+\cdots+d_n$. Define the d-dimensional random vector $\mathbf{Y}=\left(\mathbf{X}_1, \ldots, \mathbf{X}_n\right)$ by putting all the coordinates of the $\mathbf{X}_i$ s together. Denote the joint cumulative distribution functions of these random vectors by $F_{\mathbf{Y}}, F_{\mathbf{X}_1}, \ldots, F_{\mathbf{X}_n}$. Then $\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n$ are independent if and only if
$$
F_{\mathbf{Y}}\left(\mathbf{x}_1, \ldots, \mathbf{x}_n\right)=F_{\mathbf{X}_1}\left(\mathbf{x}_1\right) \cdot F_{\mathbf{X}_2}\left(\mathbf{x}_2\right) \cdots F_{\mathbf{x}_n}\left(\mathbf{x}_n\right)
$$
for all vectors $\mathbf{x}_1 \in \mathbb{R}^{d_1}, \mathbf{x}_2 \in \mathbb{R}^{d_2}, \ldots, \mathbf{x}_n \in \mathbb{R}^{d_n}$.
\end{theorem}

\section{Independent and identically distributed random variables}
\begin{definition}
   Random variables $X_1, X_2, X_3, \ldots$ are independent and identically distributed (abbreviated i.i.d.) if they are independent and each $X_k$ has the same probability distribution. That is, $X_k \stackrel{d}{=} X_{\ell}$ for any two indices $k, \ell . $
\end{definition}
\section{Independence of jointly absolutely continuous random variables.}

\begin{theorem}
    Theorem 3.11. Let $X_1, \ldots, X_d$ be random variables on the same sample space. Assume that for each $j=1,2, \ldots, d, X_j$ has density function $f_{X_j}$.
(a) If $X_1, \ldots, X_d$ have joint density function $f$ given by
$$
f\left(x_1, x_2, \ldots, x_d\right)=f_{X_1}\left(x_1\right) f_{X_2}\left(x_2\right) \cdots f_{X_d}\left(x_d\right)
$$
then $X_1, \ldots, X_d$ are independent.\\\\
(b) Suppose $X_1, \ldots, X_d$ are independent. Then they are jointly absolutely continuous with joint density function
$$
f\left(x_1, x_2, \ldots, x_d\right)=f_{X_1}\left(x_1\right) f_{X_2}\left(x_2\right) \cdots f_{X_d}\left(x_d\right)
$$
\end{theorem}

\section{Finding Independence}

\begin{theorem}
(a)
Suppose $X_1, \ldots, X_n$ are independent random variables and for each index $i, g_i$ is a function on the range of $X_i$. Then the random variables $g_1\left(X_1\right), g_2\left(X_2\right), \ldots, g_n\left(X_n\right)$ are independent.

One needs to assume that $g_1, \ldots, g_n$ are measurable, but this does not come up usually in applications.\\ \\
(b) Let $\left\{X_k\right\}_{k \geq 1}$ be a finite or infinite sequence of independent random variables. Let $0=k_0<k_1<\cdots<k_n$ be integers. Let $g_1, \ldots, g_n$ be functions such that $g_j$ is defined on the range of the random vector $\left(X_{k_{j-1}+1}, \ldots, X_{k_j}\right)$. Define new random variables $Y_j=g_j\left(X_{k_{j-1}+1}, \ldots, X_{k_j}\right)$ for $j=1, \ldots, n$. Then the random variables $Y_1, \ldots, Y_n$ are independent.
\end{theorem}

\section{Binomial Distribution}

\begin{definition}{Binomial Distribution}
Let $n$ be a positive integer and $0 \leq p \leq 1$. A random variable $X$ has the binomial distribution with parameters $n$ and $p$ if the possible values of $X$ are $\{0,1, \ldots, n\}$ and the probabilities are
$$
P(X=k)=\left(\begin{array}{l}
n \\
k
\end{array}\right) p^k(1-p)^{n-k} \quad \text { for } k=0,1, \ldots, n .
$$
Abbreviate this by $X \sim \operatorname{Bin}(n, p)$.
\end{definition}

\section{Geometric Distribution}
\begin{definition}{Geometric Distribution}
    Let $ 0 < p \le 1$. A random variable X has the $\textbf{geometric distribution} $ with success parameter $p$ if the set of possible values of X is $\mathbb{Z}_>0$ and X satisfies $P(X = k) = (1-p)^{k-1}p$ for positive integers k. Abbreviate this by $X \sim \operatorname{Geom}(p)$.
\end{definition}

\section{Negative Binomial Distribution}

\begin{definition}{Negative binomial distribution}
 Let $k$ be a positive integer and $0<p \leq 1$. A random variable $X$ has the negative binomial distribution with parameters $(k, p)$ if the set of possible values of $X$ is the set of integers $\mathbb{Z}_{\geq k}=\{k, k+1, k+2, \ldots\}$ and the probability mass function is
    $$
    P(X=n)=\left(\begin{array}{l}
    n-1 \\
    k-1
    \end{array}\right) p^k(1-p)^{n-k} \quad \text { for } n \in \mathbb{Z}_{\geq k}
    $$
    Abbreviate this by $X \sim \operatorname{Negbin}(k, p)$.

\end{definition}
\begin{note}
    The Negbin $(1, p)$ distribution is the same as the $\operatorname{Geom}(p)$ distribution.
\end{note}

\begin{corollary}
 Consider a sequence of independent trials with success probability $0<p \leq 1$. Let $N_k$ be the number of trials needed for the $k$ th success. Set $Y_1=N_1$ and $Y_k=N_k-N_{k-1}$ for $k \geq 2$. Then the random variables $Y_1, Y_2, Y_3, \ldots$ are i.i.d. In particular, $N_k-N_{k-1} \sim \operatorname{Geom}(p)$ for each $k \geq 2$.
\end{corollary}
\begin{note}
    Can think of negative binomial as sum of geometric random variables.
\end{note}
\section{Possion Distribution}
\begin{definition}{Possion Distribution}
   Let $\lambda>0$. A random variable $X$ has the Poisson distribution with parameter $\lambda$ if the possible values of $X$ are the nonnegative integers and the probability mass function is
    $$
    P(X=k)=e^{-\lambda} \frac{\lambda^k}{k !} \quad \text { for } k \in\{0,1,2, \ldots\}
    $$
    Abbreviate this by $X \sim$ Poisson $(\lambda)$.
\end{definition}
\begin{theorem}
 Fix $\lambda>0$. For positive integers $n$ for which $\lambda / n<1$, let $S_n \sim$ $\operatorname{Bin}(n, \lambda / n)$. Then
$$
\lim _{n \rightarrow \infty} P\left(S_n=k\right)=e^{-\lambda} \frac{\lambda^k}{k !} \quad \text { for all } \quad k \in \mathbb{Z}_{\geq 0}
$$
\end{theorem}
\section{Exponential Distributions}
\begin{definition}
 Let $0<\lambda<\infty$. A random variable $X$ has the exponential distribution with parameter $\lambda$ if $X$ has density function
$$
f(x)= \begin{cases}\lambda e^{-\lambda x}, & x \geq 0 \\ 0, & x<0\end{cases}
$$
on the real line. Abbreviate this by $X \sim \operatorname{Exp}(\lambda)$.
\end{definition}{Exponential Distribution}
\begin{theorem}
 Suppose that $X \sim \operatorname{Exp}(\lambda)$. Then for any $s, t>0$, $(3.27)$
$$
P(X>t+s \mid X>t)=P(X>s) .
$$
\end{theorem}
\begin{note}
This is also called the memoryless property.
\end{note}
\begin{theorem}
 Fix $\lambda>0$. Consider $n$ large enough so that $\lambda / n<1$. Suppose that for each $n$, the random variable $T_n$ satisfies $n T_n \sim \operatorname{Geom}(\lambda / n)$. Then
$$
\lim _{n \rightarrow \infty} P\left(T_n>t\right)=e^{-\lambda t} \quad \text { for all real } t \geq 0
$$
\end{theorem}
\section{Multinomial distribution}
\begin{definition}
Let $n$ and $r$ be positive integers and let $p_1, p_2, \ldots, p_r$ be positive reals such that $p_1+p_2+\cdots+p_r=1$. The random vector $\mathbf{X}=\left(X_1, \ldots, X_r\right)$ has the multinomial distribution with parameters $n, r$ and $p_1, \ldots, p_r$ if the possible values of $\mathbf{X}$ are integer vectors $\left(k_1, \ldots, k_r\right)$ such that $k_j \geq 0$ and $k_1+\cdots+k_r=n$, and the joint probability mass function is given by
$$
P\left(X_1=k_1, X_2=k_2, \ldots, X_r=k_r\right)=\left(\begin{array}{c}
n \\
k_1, k_2, \ldots, k_r
\end{array}\right) p_1^{k_1} p_2^{k_2} \cdots p_r^{k_r}
$$
where the multinomial coefficient is defined by
$$
\left(\begin{array}{c}
n \\
k_1, k_2, \ldots, k_r
\end{array}\right)=\frac{n !}{k_{1} ! k_{2} ! \cdots k_{r} !}
$$
Abbreviate this by $\left(X_1, \ldots, X_r\right) \sim \operatorname{Mult}\left(n, r, p_1, \ldots, p_r\right)$.
\end{definition}

\section{Gamma Distribution}
\begin{definition}
Let $r, \lambda>0$. A random variable $X$ has the gamma distribution with parameters $(r, \lambda)$ if $X$ is nonnegative and has probability density function
$$
f_X(x)=\frac{\lambda^r x^{r-1}}{\Gamma(r)} e^{-\lambda x} \quad \text { for } x>0,
$$
and $f_X(x)=0$ for $x \leq 0$. We abbreviate this $X \sim \operatorname{Gamma}(r, \lambda)$.
\end{definition}
\section{Convolution}

\begin{definition}
    As a mathematical concept, convolution is a way of multiplying functions to produce new functions. The operation is denoted by *. The convolution of two functions on the real line is the function $f * g$ whose value at $x$ is defined by
$$
(f * g)(x)=\int_{-\infty}^{\infty} f(y) g(x-y) d y,
$$
provided of course that these integrals are well-defined for all $x \in \mathbb{R}$. Change of variable from $y$ to $x-y$ shows that convolution is symmetric: $f * g=g * f$.
\end{definition}

\begin{theorem}{Discrete}
If $X$ and $Y$ are independent $\mathbb{Z}$-valued random variables with probability mass functions $p_X$ and $p_Y$, then the probability mass function of $X+Y$ is
$$
p_{X+Y}(n)=p_X * p_Y(n)=\sum_{k \in \mathbb{Z}} p_X(k) p_Y(n-k)=\sum_{\ell \in \mathbb{Z}} p_X(n-\ell) p_Y(\ell)
$$
\end{theorem}

\begin{theorem}{Continuous}
If $X$ and $Y$ are independent absolutely continuous random variables with density functions $f_X$ and $f_Y$ then the density function of $X+Y$ is
(3.39) $f_{X+Y}(z)=f_X * f_Y(z)=\int_{-\infty}^{\infty} f_X(x) f_Y(z-x) d x=\int_{-\infty}^{\infty} f_X(z-x) f_Y(x) d x$.
\end{theorem}

\section{Exchangebale Random Variables}
\begin{definition}
 Random variables $X_1, \ldots, X_n$ are exchangeable if for any permutation $\sigma$ on $\{1,2, \ldots, n\}$, the joint distribution of $\left(X_{\sigma(1)}, X_{\sigma(2)}, \ldots, X_{\sigma(n)}\right)$ is the same as the joint distribution of $\left(X_1, X_2, \ldots, X_n\right)$. In other words, permuting the random variables does not change the joint distribution.
\end{definition}
\begin{note}
    Verifying exchangeability boils down to checking that either the joint cumulative distribution function, the joint probability mass function, or the joint density function is a symmetric function. (A function is symmetric if its value is not altered by permuting its arguments.) These cases are collected in the next theorem.
\end{note}

\begin{theorem}
    (i) The random variables $X_1, \ldots, X_n$ are exchangeable if for any permutation $\sigma$ on $\{1, \ldots, n\}$ and for any choice of real numbers $x_1, x_2, \ldots, x_n$ we have
$$
P\left(X_1 \leq x_1, \ldots, X_n \leq x_n\right)=P\left(X_1 \leq x_{\sigma(1)}, \ldots, X_n \leq x_{\sigma(n)}\right)
$$
(ii) Suppose $X_1, \ldots, X_n$ are discrete. Then exchangeability is equivalent to
$$
P\left(X_1=x_1, \ldots, X_n=x_n\right)=P\left(X_1=x_{\sigma(1)}, \ldots, X_n=x_{\sigma(n)}\right)
$$
for all permutations $\sigma$ on $\{1, \ldots, n\}$ and for all choices of real numbers $x_1, x_2, \ldots, x_n$.\\
(iii) Suppose $X_1, \ldots, X_n$ are jointly absolutely continuous with joint density function $f_{X_1, \ldots, X_n}$. Then exchangeability is equivalent to having the identity
$$
f_{X_1, \ldots, X_n}\left(x_1, \ldots, x_n\right)=f_{X_1, \ldots, X_n}\left(x_{\sigma(1)}, \ldots, x_{\sigma(n)}\right)
$$
for all permutations $\sigma$ on $\{1, \ldots, n\}$ and for all choices of real numbers $x_1, x_2, \ldots, x_n$, except possibly on a set of zero volume.
\end{theorem}

\begin{corollary}{Producing new exchangeable random variables}
Suppose that $X_1, \ldots, X_n$ are exchangeable.\\
(i) If $1 \leq m \leq n$ then $X_1, \ldots, X_m$ are also exchangeable.\\
(ii) Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be a function and define $Y_k=g\left(X_k\right)$. Then the random variables $Y_1, \ldots, Y_n$ are also exchangeable.
\end{corollary}

\begin{corollary}{Applications of exchangeability}
 If $X_1, \ldots X_n$ are exchangeable and $k_1, \ldots, k_m$ are distinct numbers from $\{1,2, \ldots, n\}$ then the following statements hold.
(i) $X_{k_1}, \ldots, X_{k_m}$ have the same joint distribution as $X_1, \ldots, X_m$.
(ii) For all Borel sets $B \subset \mathbb{R}^m$,
$$
P\left(\left(X_{k_1}, \ldots, X_{k_m}\right) \in B\right)=P\left(\left(X_1, \ldots, X_m\right) \in B\right)
$$
\end{corollary}
\chapter{Simple Random Walk}
\begin{quotation}
[Upon proving that the best betting stragegy for Gambler's Ruin was to bet all on the first trial.] It is true that a man who does this is a fool. I have only proved that a man who does anything else is an even bigger fool.    \hfill ------- Coolidge, Julian Lowell 
\end{quotation}

\section{Ganbler's Ruin}
\begin{example}{Gambler's Ruin}
 You play repeatedly the following gamble. A fair coin is tossed: heads you win a dollar, tails you lose a dollar. You start playing with $x$ dollars in your pocket. You choose a target $M>x$. Then you play until you either reach $M$ dollars or lose all your money. A question of obvious interest: what is the probability that you reach $M$ dollars before going broke?
 
$$
\begin{gathered}
P_x(\text { reach } M \text { before } 0)=\frac{1}{2} P_x(\text { reach } M \text { before } 0 \mid \text { first flip heads }) \\
+\frac{1}{2} P_x(\text { reach } M \text { before } 0 \mid \text { first flip tails }) \\
=\frac{1}{2} P_{x+1}(\text { reach } M \text { before } 0)+\frac{1}{2} P_{x-1}(\text { reach } M \text { before } 0) .
\end{gathered}
$$
$$
1=p_M-p_0=\sum_{x=1}^M\left(p_x-p_{x-1}\right)=M\left(p_k-p_{k-1}\right)
$$
from which $p_k-p_{k-1}=1 / M$. Then for any $x$,
$$
p_x=p_x-p_0=\sum_{k=1}^x\left(p_k-p_{k-1}\right)=\frac{x}{M}
$$
\end{example}

\begin{definition}
Fix $p \in(0,1)$. Let $X_1, X_2, X_3, \ldots$ be i.i.d. random variables with $P\left(X_i=1\right)=p$ and $P\left(X_i=-1\right)=1-p$. Let $S_0$ be an integer. (If $S_0$ is also random, then $S_0$ is independent of the random variables $\left\{X_i\right\}$.) For $n \geq 1$ define
$$
S_n=S_0+X_1+X_2+\cdots+X_n
$$
The random sequence $S_0, S_1, S_2, \ldots$ is the simple random walk (SRW) with initial position $S_0$. If an initial position is not specified, then $S_0=0$.

If $p=\frac{1}{2}$ then $\left\{S_n\right\}$ is called symmetric simple random walk (SSRW), while if $p \neq \frac{1}{2}$, then $\left\{S_n\right\}$ is asymmetric simple random walk.
\end{definition}

\begin{theorem}
Fix integers $0<x<M$. Consider $S S R W\left\{S_n\right\}_{n \geq 0}$ with nonrandom initial state $S_0=x$. Then
$$
P\left(S_n \text { visits point } M \text { before visiting } 0\right)=\frac{x}{N} .
$$
\end{theorem}
\begin{theorem}
 For times $0 \leq m<n$ and points $a, b \in \mathbb{Z}, S R W$ with initial point $S_0=0$ satisfies
$$
P\left(S_{n+m}=a+b \mid S_m=a\right)=P\left(S_n=b\right)
$$
\end{theorem}
\section{Symmetric simple random walk}
\begin{theorem}{Reflection principle}
 Let $a, b$ be integers with $b>\max (0, a)$. The number of paths of length $n$ that go from 0 to a and visit point $b$ along the way is equal to $N_{n, 0 \rightarrow 2 b-a}$. In particular, for $S S R W$,
$$
P\left(S_n=a, S_k=b \text { for some } k=0, \ldots, n\right)=P\left(S_n=2 b-a\right) \text {. }
$$
\end{theorem}
\section{Distribution of the running maximum}

The running maximum of the random walk is defined by $M_n=\max \left(0, S_1, \ldots, S_n\right)$ for $n=0,1,2, \ldots$ It is always nonnegative. We find its distribution.
\begin{theorem}
    For $r \in \mathbb{Z}_{\geq 0}$, the running maximum of symmetric $S R W$ satisfies
$$
P\left(M_n=r\right)=P\left(S_n=r\right)+P\left(S_n=r+1\right) .
$$
Note that one of the terms on the right is always zero depending on the parity of $n-r$.
\end{theorem}

\begin{note}
    Will include more on this later in stochastic process.
\end{note}
\chapter{Expectation}
\section{Definition of expectation}
\begin{definition}{Arbitrary real-valued random variables}
    All definitions of the expected value may be expressed in the language of measure theory. In general, if $X$ is a real-valued random variable defined on a probability space $(\Omega, \Sigma, \mathrm{P})$, then the expected value of $X$, denoted by $\mathrm{E}[X]$, is defined as the Lebesgue integral
$$
\mathrm{E}[X]=\int_{\Omega} X d \mathrm{P}
$$
\end{definition}

\begin{note}
    This is a generalized definition which required Lebesgue Integral. I will direct the readers who are interested in this definiton to \href{https://en.wikipedia.org/wiki/Expected_value#Arbitrary_real-valued_random_variables}{here} .  Another definiton is given below.
\end{note}

\begin{definition}{3 step construction}
(Definition of the expectation $\mathrm{E}[X]$). Consider random variables on some probability space $(\Omega, \mathcal{F}, P)$.\\

Step 1. A simple random variable is a discrete random variable with finitely many values. Let $X$ be a nonnegative simple random variable. Then $X$ is of the form
$$
X(\omega)=\sum_{i=1}^m \alpha_i I_{A_i}(\omega)
$$

where $\alpha_1, \ldots, \alpha_m$ are its distinct nonnegative real values and the events $A_i=\{X=\alpha_i\}$ form a partition of $\Omega$. Define the expectation of $X$ by
$$
\mathrm{E}[X]=\sum_{i=1}^m \alpha_i P\left(A_i\right)=\sum_{i=1}^m \alpha_i P\left(X=\alpha_i\right)
$$

Step 2. Let $X$ be a $[0, \infty]$-valued random variable on $\Omega$. That is, the values $X(\omega)$ are nonnegative reals and possibly $\infty$. Then the expectation of $X$ is defined by
$\mathrm{E}[X]=\sup \{E Y: Y$ is a simple random variable on $\Omega$ such that
$$
0 \leq Y(\omega) \leq X(\omega) \text { for all } \omega \in \Omega\}
$$
This defines a value $\mathrm{E}[X] \in[0, \infty]$.\\

Step 3. Let $X$ be a $[-\infty, \infty]$-valued random variable on $\Omega$. That is, the values $X(\omega)$ are real numbers, $\infty$ or $-\infty$. The random variable $X$ is decomposed into its positive part $X^{+}$and negative part $X^{-}$by defining $X^{+}(\omega)=X(\omega) \vee 0$ and $X^{-}(\omega)=(-X(\omega)) \vee 0$. Random variables $X,|X|, X^{+}$and $X^{-}$satisfy the identities
$$
X(\omega)=X^{+}(\omega)-X^{-}(\omega) \text { and }|X(\omega)|=X^{+}(\omega)+X^{-}(\omega)
$$
The expectations $E\left(X^{+}\right)$and $E\left(X^{-}\right)$were defined in Step 2. If at least one of them is finite, the expectation of $X$ is defined by
$$
\mathrm{E}[X]=E\left(X^{+}\right)-E\left(X^{-}\right)
$$
In this case we say that $\mathrm{E}[X]$ is well-defined. If both $E\left(X^{+}\right)$and $E\left(X^{-}\right)$are infinite, then $\mathrm{E}[X]$ is not defined.
\end{definition}
\begin{note}
    $\vee$ is an alternative notation for maximum
\end{note}

\begin{theorem}
(a)
Suppose $X$ is a discrete random variable. Then the following expectations have well-defined values in $[0, \infty]$ :
$$
E\left(X^{+}\right)=\sum_{k \geq 0} k P(X=k) \quad \text { and } \quad E\left(X^{-}\right)=\sum_{k \leq 0}(-k) P(X=k) .
$$
If at least one of these values is finite, then $E X$ is well defined as the difference:
$$
E X=\sum_{k \geq 0} k P(X=k)-\sum_{k \leq 0}(-k) P(X=k)
$$\\

(b) Suppose $X$ is an absolutely continuous random variable with density function $f$. Then the following expectations have well-defined values in $[0, \infty]$ :
$$
E\left(X^{+}\right)=\int_0^{\infty} x f(x) d x \quad \text { and } \quad E\left(X^{-}\right)=\int_{-\infty}^0(-x) f(x) d x
$$
If at least one of these values is finite, then $E X$ is well defined as the difference:
$$
E X=\int_0^{\infty} x f(x) d x-\int_{-\infty}^0(-x) f(x) d x
$$
\end{theorem}
\begin{corollary}
When well-defined the below definition also holds. \\
(a) For a discrete random variable
$$
E(X)=\sum_k k P(X=k)
$$
where the sum ranges over all the possible values $k$ of $X$.\\
(b) For an absolutely continuous random variable $X$ with density function $f$
$$
E[X]=\int_{-\infty}^{\infty} x f(x) d x
$$
\end{corollary}
\section{Theorems and propositions for expectation}
\begin{theorem}
Suppose $P(X \geq 0)=1$. Then
$$
E(X)=\int_0^{\infty} P(X>s) d s
$$
In the particular discrete case where $P\left(X \in \mathbb{Z}_{\geq 0}\right)=1$, the formula can also be expressed as
$$
E(X)=\sum_{k=0}^{\infty} P(X>k)
$$
\end{theorem}

\begin{theorem}
Let $\mathbf{X}=\left(X_1, \ldots, X_d\right.$ ) be a random vector (or a scalar random variable, in case $d=1$ ) and $g$ a real-valued function defined on the range of $\mathbf{X}$. Assume that either $g \geq 0$ or that $g(\mathbf{X})$ is absolutely integrable, so that the expectation $E[g(\mathbf{X})]$ is well-defined. Then we have the following formulas for this expectation.
(a) Suppose $X_1, \ldots, X_d$ are discrete random variables. Then
$$
E[g(\mathbf{X})]=\sum_{\mathbf{k}} g(\mathbf{k}) P(\mathbf{X}=\mathbf{k})
$$
where $\mathbf{k}$ ranges over the possible values of $\mathbf{X}$.
(b) Suppose $X_1, \ldots, X_d$ are jointly continuous random variables with joint density function $f$. Then
$$
E[g(\mathbf{X})]=\int_{\mathbb{R}^d} g(\mathbf{x}) f(\mathbf{x}) d \mathbf{x}
$$
\end{theorem}

\begin{proposition}
 Let $X$ and $Y$ be random variables on $(\Omega, \mathcal{F}, P)$. Assume their expectations are well-defined. Then their expectations have the following properties.\\
(i) Linearity: if $E X$ and $E Y$ are finite, then for any real numbers a,b,c,
$$
E[a X+b Y+c]=a E(X)+b E(Y)+c .
$$
(ii) Monotonicity: if $P(X \leq Y)=1$, then $E X \leq E Y$. If $P(X=Y)=1$ then $E X=E Y$\\
(iii) $|E X| \leq E|X|$.\\
(iv) Suppose $P(X \geq 0)=1$ and $E X=0$. Then $P(X=0)=1$.
\end{proposition}

\begin{theorem}
    
    Let $X$ be a nonnegative random variable and $r \in(1, \infty)$. Assume that $E\left[X^r\right]<\infty$. Then $E X<\infty$.
\end{theorem}
\begin{note}
    This is an analogy for convergence in $L_p$ space.
\end{note}

\section{Variance}
    \begin{definition}
        The variance is important enough to be highlighted as a formula. If $X$ has finite mean $\mu=E X$, then its variance is
$$
\operatorname{Var}(X)=\sigma^2=E\left[(X-\mu)^2\right]
$$
Expansion of the square inside the brackets and linearity of expectation give an alternative formula for the variance:
$$
\begin{aligned}
\operatorname{Var}(X) & =E\left[X^2-2 \mu X+\mu^2\right]=E\left[X^2\right]-2 \mu E X+\mu^2 \\
& =E\left[X^2\right]-\mu^2
\end{aligned}
$$
    \end{definition}

\begin{theorem}
    (i) Suppose $X$ has finite variance and $a, b$ are real numbers. Then $\operatorname{Var}(a X+b)=$ $a^2 \operatorname{Var}(X)$\\
(ii) $\operatorname{Var}(X)=0$ if and only if there exists a real number $c$ such that $P(X=c)=1$. When this happens, $c$ is the mean of $X$.
\end{theorem}

\section{Linearity of expectation}

\begin{theorem}
    let $\mathbf{X}_i, 1 \leq i \leq n$, be random vectors defined on $(\Omega, \mathcal{F}, P)$ and for each index $i$ let $g_i$ be a real-valued function defined on the range of $X_i$. Then, as long as the expectations below are finite,
$$
\begin{aligned}
E\left[g_1\left(\mathbf{X}_1\right)+g_2\left(\mathbf{X}_2\right)\right. & \left.+\cdots+g_n\left(\mathbf{X}_n\right)\right] \\
= & E\left[g_1\left(\mathbf{X}_1\right)\right]+E\left[g_2\left(\mathbf{X}_2\right)\right]+\cdots+E\left[g_n\left(\mathbf{X}_n\right)\right]
\end{aligned}
$$
\end{theorem}

\begin{corollary}
 Let $X_1, X_2, \ldots, X_n$ be random variables defined on the same probability space. Then
$$
E\left[X_1+X_2+\cdots+X_n\right]=E\left[X_1\right]+E\left[X_2\right]+\cdots+E\left[X_n\right]
$$
provided all the expectations on both sides are finite.
\end{corollary}
\begin{note}
    The caveat in the theorem about all expectations being finite is needed.
\end{note}
\section{Expectation and independence}
\begin{theorem}
Suppose $X_1, \ldots, X_n$ are independent random variables. Then for all functions $g_1, \ldots, g_n$ for which the expectations below are well-defined,
$$
E\left[\prod_{k=1}^n g_k\left(X_k\right)\right]=\prod_{k=1}^n E\left[g_k\left(X_k\right)\right]
$$
\end{theorem}

\begin{theorem}
Assume the random variables $X_1, \ldots, X_n$ are independent and have finite variances. Then
$$
\operatorname{Var}\left(X_1+X_2+\cdots+X_n\right)=\operatorname{Var}\left(X_1\right)+\operatorname{Var}\left(X_2\right)+\cdots+\operatorname{Var}\left(X_n\right)
$$
\end{theorem}
\section{Covariance and correlation}
\begin{definition}
Let $X$ and $Y$ be random variables defined on the same sample space with expectations $\mu_X$ and $\mu_Y$. The covariance of $X$ and $Y$ is
$$
\operatorname{Cov}(X, Y)=E\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]
$$
if the expectation on the right is finite.
By expanding the product inside the expectation, we get an alternative formula for the covariance:
$$
\begin{aligned}
\operatorname{Cov}(X, Y) & =E\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]=E\left[X Y-\mu_X Y-\mu_Y X+\mu_X \mu_Y\right] \\
& =E[X Y]-\mu_X E[Y]-\mu_Y E[X]+\mu_X \mu_Y \\
& =E[X Y]-\mu_X \mu_Y
\end{aligned}
$$
\end{definition}
\begin{proposition}
 The following statements hold when the covariances are welldefined.\\
(i) $\operatorname{Cov}(X, Y)=\operatorname{Cov}(Y, X)$.\\
(ii) $\operatorname{Cov}(a X+b, Y)=a \operatorname{Cov}(X, Y)$ for real numbers $a, b$.\\
(iii) Covariance is bilinear: namely, for random variables $X_i$ and $Y_j$ and real numbers $a_i$ and $b_j$
$$
\operatorname{Cov}\left(\sum_{i=1}^m a_i X_i, \sum_{j=1}^n b_j Y_j\right)=\sum_{i=1}^m \sum_{j=1}^n a_i b_j \operatorname{Cov}\left(X_i, Y_j\right)
$$
\end{proposition}
\begin{theorem}
 Let $X_1, \ldots, X_n$ be random variables with finite variances and covariances. Then
$$
\operatorname{Var}\left(\sum_{i=1}^n X_i\right)=\sum_{i=1}^n \operatorname{Var}\left(X_i\right)+2 \sum_{1 \leq i<j \leq n} \operatorname{Cov}\left(X_i, X_j\right)
$$
\end{theorem}
\begin{definition}
Let $X$ and $Y$ be random variables such that $\operatorname{Cov}(X, Y)$ is finite, $0<\operatorname{Var}(X)<\infty$ and $0<\operatorname{Var}(Y)<\infty$. The correlation coefficient of $X$ and $Y$ is defined by
$$
\operatorname{Corr}(X, Y)=\frac{\operatorname{Cov}(X, Y)}{\sqrt{\operatorname{Var}(X)} \sqrt{\operatorname{Var}(Y)}}
$$
\end{definition}
\begin{proposition}
Assume that $\operatorname{Cov}(X, Y)$ is finite, $0<\operatorname{Var}(X)<\infty$ and $0<$ $\operatorname{Var}(Y)<\infty$. The correlation coefficient has these properties.\\
(i) $-1 \leq \operatorname{Corr}(X, Y) \leq 1$.\\
(ii) $\operatorname{Corr}(X, Y)=1$ if and only if there exist $a>0$ and $b \in \mathbb{R}$ such that $Y=a X+b$.\\
(iii) $\operatorname{Corr}(X, Y)=-1$ if and only if there exist $a<0$ and $b \in \mathbb{R}$ such that $Y=$ $a X+b$
\end{proposition}
\begin{note}
    (i) follows from Cauchy-Schwarz inequality.
\end{note}

\chapter{Law of large numbers}
\section{Markov and Chebyshev inequalities}
\begin{lemma}{ Markov's inequality}
 Let $X$ be a $[0, \infty]$-valued random variable and $c>0$. Then
$$
P(X \geq c) \leq \frac{E X}{c} .
$$
This inequality is useful only if $c>E X$.
\end{lemma}

\begin{lemma}{Chebyshev's inequality}
 Let $X$ be a random variable with finite mean $\mu$ and finite variance $\sigma^2$. Then for any real $c>0$ we have
$$
P(|X-\mu| \geq c) \leq \frac{\sigma^2}{c^2}
$$
\end{lemma}

\begin{theorem}{  Weak law of large numbers, WLLN}
 Let $\left\{X_k\right\}_{k \geq 1}$ be i.i.d. random variables with finite mean $\mu=E\left[X_k\right]$ and finite variance ${\sigma^2}=\operatorname{Var}\left(X_k\right)$. Let $S_n=X_1+\cdots+X_n$. Then for any fixed $\varepsilon>0$ we have
$$
\lim _{n \rightarrow \infty} P\left(\left|\frac{S_n}{n}-\mu\right| \geq \varepsilon\right)=0
$$
\end{theorem}
\begin{note}
    This is the same with $\frac{S_n}{n}$ converge in probability to $\mu$.
\end{note}

\section{Convergence in probability and Almost surely convergence}
\begin{definition}
Let $\left\{X_n\right\}_{n \geq 1}$ be a sequence of random variables and $X$ another random variable, all defined on the same probability space $(\Omega, \mathcal{F}, P)$.\\

(a) We say that $X_n$ converges to $X$ in probability as $n \rightarrow \infty$ if, for each $\varepsilon>0$
$$
\lim _{n \rightarrow \infty} P\left(\left|X_n-X\right| \geq \varepsilon\right)=0
$$
A common abbreviation is $X_n \stackrel{P}{\rightarrow} X$.\\

(b) We say that $X_n$ converges to $X$ almost surely or with probability one if there is an event $\Omega_0 \subset \Omega$ such that $P\left(\Omega_0\right)=1$ and for all $\omega \in \Omega_0, X_n(\omega) \rightarrow X(\omega)$ as $n \rightarrow \infty$. Common abbreviations are $X_n \rightarrow X$ a.s. and $X_n \rightarrow X$ w.p.1.\\
The definition of almost sure convergence $X_n \rightarrow X$ can be stated succinctly as $P\left\{\omega: \lim _{n \rightarrow \infty} X_n(\omega)=X(\omega)\right\}=1$ or even more tersely as $P\left(X_n \rightarrow X\right)=1$. 
\end{definition}

\begin{theorem}
Suppose $X_n \rightarrow X$ almost surely. Then also $X_n \rightarrow X$ in probability.
\end{theorem}
\section{Borel-Cantelli lemma}
\begin{lemma}{Borel-Cantelli lemma}
    Let $\left\{A_n\right\}_{n \geq 1}$ be a sequence of events, all defined on the same sample space. Suppose $\sum_{n=1}^{\infty} P\left(A_n\right)<\infty$. Then
    $$
    P\left\{\omega: \omega \in A_n \text { for infinitely } \operatorname{many} n\right\}=0
    $$ 
\end{lemma}

\section{Almost sure convergence from the Borel-Cantelli lemma}
The next theorem encapsulates the most common strategy for proving almost sure convergence with the Borel-Cantelli lemma.
\begin{theorem}
Let $\left\{X_n\right\}_{n \geq 1}$ and $X$ be random variables on $(\Omega, \mathcal{F}, P)$. Suppose that for all $\varepsilon>0$
$$
\sum_{n=1}^{\infty} P\left(\left|X_n-X\right| \geq \varepsilon\right)<\infty .
$$
Then $X_n \rightarrow X$ almost surely.
\end{theorem}
\section{ Strong law of large numbers}
\begin{theorem}
    Let $\left\{X_k\right\}_{k \geq 1}$ be i.i.d. random variables with finite mean $\mu=E\left[X_k\right]$. Let $S_n=X_1+\cdots+X_n$. Then
$$
P\left\{\omega: \lim _{n \rightarrow \infty} \frac{S_n(\omega)}{n}=\mu\right\}=1 .
$$
In other words, $S_n / n \rightarrow \mu$ almost surely.
\end{theorem}

\chapter{Limits in distribution}
\section{Converge in distribution}
The definition of a limit in distribution is given below. In contrast with almost sure convergence and convergence in probability, for convergence in distribution the random variables do not have to be defined on the same sample space.
\begin{definition}

 Suppose that for each positive integer $n, X_n$ is a random variable with cumulative distribution function $F_n$. Let $X$ be a random variable with cumulative distribution function $F$. Then $X_n$ converges to $X$ in distribution if $F_n(x) \rightarrow F(x)$ as $n \rightarrow \infty$ at each point $x$ where $F$ is continuous.
\end{definition}

\begin{theorem}
 Suppose $X_n \rightarrow X$ in probability. Then also $X_n \rightarrow X$ in distribution.
\end{theorem}
\section{Gaussian Distribution}

In preparation for the central limit theorem, we introduce the Gaussian, or normal distribution.
\begin{definition}
Let $\mu$ be real and $\sigma>0$. A random variable $X$ has the normal distribution with mean $\mu$ and variance $\sigma^2$ if $X$ has density function
$$
f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
$$
on the real line. Abbreviate this by $X \sim \mathcal{N}\left(\mu, \sigma^2\right)$.
\end{definition}
\begin{note}
    There is a great 3blue1brown video explaining why this is a PDF. 
\end{note}
\begin{theorem}
Let $Z \sim \mathcal{N}(0,1)$. Then $E(Z)=0$ and $\operatorname{Var}(Z)=E\left(Z^2\right)=1$.
\end{theorem}

\begin{proposition}
Let $\mu$ be real, $\sigma>0$, and $X \sim \mathcal{N}\left(\mu, \sigma^2\right)$.\\
(i) Let $a \neq 0$, b real, and $Y=a X+b$. Then $Y \sim \mathcal{N}\left(a \mu+b, a^2 \sigma^2\right)$.\\
(ii) $Z=\frac{X-\mu}{\sigma}$ is a standard normal random variable.
\end{proposition}
\section{ Central limit theorem}
\begin{theorem}{ Central limit theorem}

 Suppose $X_1, X_2, X_3, \ldots$ are i.i.d. random variables with finite mean $E\left[X_1\right]=\mu$ and finite variance $\operatorname{Var}\left(X_1\right)=\sigma^2$. Let $S_n=$ $X_1+\cdots+X_n$. Then for any fixed $-\infty \leq a \leq b \leq \infty$ we have
$$
\lim _{n \rightarrow \infty} P\left(a \leq \frac{S_n-n \mu}{\sigma \sqrt{n}} \leq b\right)=\Phi(b)-\Phi(a)=\int_a^b \frac{1}{\sqrt{2 \pi}} e^{-\frac{y^2}{2}} d y
$$
\end{theorem}

\section{Normal Approximation of the binomial}
\begin{theorem}{Central limit theorem for Bernoulli random variables}
 Let $0<$ $p<1$ be fixed and suppose that $S_n \sim \operatorname{Bin}(n, p)$. Then for any fixed $-\infty \leq a \leq b \leq$ $\infty$
$$
\lim _{n \rightarrow \infty} P\left(a \leq \frac{S_n-n p}{\sqrt{n p(1-p)}} \leq b\right)=\Phi(b)-\Phi(a)=\int_a^b \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} d x .
$$
Another name for this theorem is the de Moivre-Laplace theorem, and it goes back to the early 1700s. We use this theorem as an imprecise approximation of binomial probabilities with Gaussian probabilities:
$$
P\left(a \leq \frac{S_n-n p}{\sqrt{n p(1-p)}} \leq b\right) \approx \Phi(b)-\Phi(a)
$$
\end{theorem}
\begin{note}
    A commonly used rule of thumb is that the approximation is good if $n p(1-p)>10$.
\end{note}
\section{Continuity correction}
\begin{corollary}
A random variable $S_n \sim \operatorname{Bin}(n, p)$ takes only integer values. Thus, if $k_1, k_2$ are integers then
$$
P\left(k_1 \leq S_n \leq k_2\right)=P\left(k_1-1 / 2 \leq S_n \leq k_2+1 / 2\right)
$$
\end{corollary}
\section{Confidence interval}
$$
\begin{aligned}
P(|\widehat{p}-p|<\varepsilon) & =P\left(\left|\frac{S_n}{n}-p\right|<\varepsilon\right)=P\left(-n \varepsilon<S_n-n p<n \varepsilon\right) \\
& =P\left(-\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}<\frac{S_n-n p}{\sqrt{n p(1-p)}}<\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}\right) .
\end{aligned}
$$
Up to this point we have used only algebra. Now comes the normal approximation:
$$
\begin{aligned}
P\left(-\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}<\frac{S_n-n p}{\sqrt{n p(1-p)}}<\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}\right) & \approx \Phi\left(\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}\right)-\Phi\left(-\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}\right) \\
& =2 \Phi\left(\frac{\varepsilon \sqrt{n}}{\sqrt{p(1-p)}}\right)-1\\
&\geq 2 \Phi(2 \varepsilon \sqrt{n})-1
\end{aligned}
$$
\section{Possion Approximation}
\begin{theorem}{Poisson approximation of the binomial with error term}
Let $S \sim$ $\operatorname{Bin}(n, p)$ and $Y \sim \operatorname{Poisson}(n p)$. Then
$$
\sum_{k=0}^{\infty}|P(S=k)-P(Y=k)| \leq 2 n p^2 .
$$
\end{theorem}
\chapter{Generating functions}
\section{3 Probability Generating functions}

\begin{definition}
    The probability generating function of a random variable $X$ is defined as the function $G_X(s)=E s^X$ (for whichever $s$ this is defined).\\

The moment generating function of a random variable $X$ is defined as the function $M_X(t)=E e^{t X}$ (for whichever $t$ this is defined).\\

The characteristic function of a random variable $X$ is defined as the function $\phi_X(t)=E e^{i t X}$
Note: $e^{i t X}$ is a complex valued random variable. The expectation of a complex random variable $Z$ is defined as $E \Re Z+i E \Im Z$. (You just take the expectation of real and imaginary parts separately.)
\end{definition}
\begin{proposition}{nth derivative}
  (a) When X is nonnegative and integer valued.$$
\left.\frac{d^k}{s^k} G_X(s)\right|_{s=0}=k ! P(X=k)
$$
Thus $G_X(s)$ identifies the distribution if $X$ is nonnegative and integer valued.\\

(b)
If $M_X(t)$ is defined in a small neighborhood of 0 then the derivatives at zero are equal to the moments (if they exist).
$$
\frac{d}{d t} E e^{t X}=E\left(e^{t X} X\right), \quad M^{\prime}(0)=E X
$$
(One would need to justify the fact that we can differentiate inside the expectation. If $M_X(t)$ is finite in a neighborhood of 0 then this is justified.) Same works for higher order derivatives (if the moments exist):
$$
M_X^{(n)}(0)=E X^n
$$
(c)
The derivatives at zero will produce the moments (if they exist).
$$
\frac{d}{d t} E e^{i t X}=E\left(e^{i t X} X\right), \quad \phi^{\prime}(0)=i E X
$$
(One would again need to justify the fact that we can differentiate inside the expectation.) Same works for higher order derivatives (if the moments exist):
$$
\phi_X^{(n)}(0)=i^n E X^n
$$
\end{proposition}


\begin{theorem}
When the moment generating function $M(t)$ of a random variable $X$ is finite in an interval around the origin, then all moments of $X$ are finite and are given by
$$
E\left(X^n\right)=M^{(n)}(0)
$$
\end{theorem}

\section{Identification of distributions with moment generating functions}
\begin{theorem}
    Let $X$ and $Y$ be two random variables with moment generating functions $M_X(t)=E\left(e^{t X}\right)$ and $M_Y(t)=E\left(e^{t Y}\right)$. Suppose there exists $\delta>0$ such that for $t \in(-\delta, \delta), M_X(t)=M_Y(t)$ and these are finite numbers. Then $X$ and $Y$ are equal in distribution.
\end{theorem}
\section{Moment generating function of a sum of independent random variables}

\begin{theorem}
 Suppose that $X$ and $Y$ are independent random variables with moment generating functions $M_X(t)$ and $M_Y(t)$. Then for all real numbers $t$
$$
M_{X+Y}(t)=M_X(t) M_Y(t) .
$$
\end{theorem}

\begin{note}
    Moment generating function can help to prove central limit theorem.
\end{note}

\chapter{Conditional Expectation}
\section{Conditional distributions}
\begin{definition}{Discrete case}
 Let $X$ and $Y$ be discrete random variables. Let $y \in \mathbb{R}$ be point such that $P(Y=y)>0$. Then the conditional probability mass function of $X$ given $Y=y$ is the function $p_{X \mid Y}(x \mid y)$ of possible values $x$ of $X$, defined as follows:
$$
p_{X \mid Y}(x \mid y)=P(X=x \mid Y=y)=\frac{P(X=x, Y=y)}{P(Y=y)}=\frac{p_{X, Y}(x, y)}{p_Y(y)}
$$
The conditional expectation of $X$ given $Y=y$ is
$$
E[X \mid Y=y]=\sum_x x p_{X \mid Y}(x \mid y)
$$
\end{definition}
\begin{definition}{Continuous case}
Let $X$ and $Y$ be jointly continuous random variables with joint density function $f_{X, Y}(x, y)$. For those $y \in \mathbb{R}$ such that $f_Y(y)>0$ we make the following definitions.

The conditional density function of $X$, given $Y=y$, is denoted by $f_{X \mid Y}(x \mid y)$ and defined as
$$
f_{X \mid Y}(x \mid y)=\frac{f_{X, Y}(x, y)}{f_Y(y)} \text {. }
$$
The conditional probability that $X \in A$, given $Y=y$, is
$$
P(X \in A \mid Y=y)=\int_A f_{X \mid Y}(x \mid y) d x .
$$
The conditional expectation of $X$, given $Y=y$, is
$$
E[X \mid Y=y]=\int_{-\infty}^{\infty} x f_{X \mid Y}(x \mid y) d x
$$
\end{definition}

\begin{theorem}
For the conditional expectation of $g(X)$ given that $Y=y$ we have the following formulas provided the expectations are well-defined.
(i) In the discrete case
$$
E[g(X) \mid Y=y]=\sum_x g(x) p_{X \mid Y}(x \mid y)
$$
for $y$ such that $P(Y=y)>0$.\\
(ii) In the jointly continuous case
$$
E[g(X) \mid Y=y]=\int_{-\infty}^{\infty} g(x) f_{X \mid Y}(x \mid y) d x .
$$
for $y$ such that $f_Y(y)>0$.
\end{theorem}

\begin{proposition}
 We have the following averaging identities. Assume that the expectations of $g(X)$ below are well-defined.
(i) In the discrete case
$$
p_X(x)=\sum_y p_{X \mid Y}(x \mid y) p_Y(y)
$$
and
$$
E[g(X)]=\sum_y E[g(X) \mid Y=y] p_Y(y) .
$$
(ii) In the jointly continuous case
$$
f_X(x)=\int_{-\infty}^{\infty} f_{X \mid Y}(x \mid y) f_Y(y) d y
$$
and
$$
E[g(X)]=\int_{-\infty}^{\infty} E[g(X) \mid Y=y] f_Y(y) d y
$$
\end{proposition}
\begin{note}
    $E[E[X|Y]] = E[X]$
\end{note}
\section{Conditioning and independence}
\begin{theorem}
Discrete random variables $X$ and $Y$ are independent if and only if $p_{X \mid Y}(x \mid y)=p_X(x)$ for all possible values $x$ of $X$, whenever $p_Y(y)>0$.

Jointly continuous random variables $X$ and $Y$ are independent if and only if $f_{X \mid Y}(x \mid y)=f_X(x)$ for all $x$, whenever $f_Y(y)>0$.
\end{theorem}

\chapter{Reference}

\href{https://people.math.wisc.edu/~valko/courses/531/531.html}{Math 531: Probability Theory} 

\end{document}