\documentclass[11pt]{elegantbook}

\title{Introduction to Probability Theory}
\subtitle{Math 531}

\author{Chris Cai/ Linrong Cai}
\institute{University of Wisconisn Madison}
\date{May 15, 2023}
\version{1.0}
\bioinfo{Instructor}{Benedek Valk√≥}

\extrainfo{The theory of probabilities is at bottom nothing but common sense reduced to calculus; it enables us to appreciate with exactness that which accurate minds feel with a sort of instinct for which of times they are unable to account.}
\vspace{30mm}
\cover{cover.png}

% modify the color in the middle of titlepage
\definecolor{customcolor}{RGB}{32,178,170}
\colorlet{coverlinecolor}{customcolor}
\colorlet{coverlinecolor}{customcolor}
\usepackage{cprotect}

\addbibresource[location=local]{reference.bib} % bib

\begin{document}

\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Events and Probability}

\section{Kolmogorov's axioms and probability space}

These are Kolmogorov's axioms for probability theory.
\begin{definition}
A $\textbf{probability space}$ is a triple $(\Omega, \mathcal{F}, P)$, with the following components. \\
(a) $\Omega$ is a set, called the $\textbf{sample space}$. \\ 
(b) $\mathcal{F}$ is a collection of subsets of $\Omega$. Members of $\mathcal{F}$ are called events. $\mathcal{F}$ is assumed to be a $\sigma$-algebra, which means that it satisfies the following three properties.\\
(b.1) $\Omega \in \mathcal{F}$. That is, the whole sample space itself is an event.\\
(b.2) If $A \in \mathcal{F}$ then $A^c \in \mathcal{F}$.\\
(b.3) If $\left\{A_k\right\}_{1 \leq k<\infty}$ is a sequence of members of $\mathcal{F}$, then their union $\bigcup_{k=1}^{\infty} A_k$ is also a member of $\mathcal{F}$.\\
(c) $P$ is a function from $\mathcal{F}$ into real numbers, called the $\textbf{probability measure}$ . $P$ satisfies the following axioms.\\
(c.1) $0 \leq P(A) \leq 1$ for each event $A \in \mathcal{F}$.\\
(c.2) $P(\varnothing)=0$ and $P(\Omega)=1$.\\
(c.3) If $\left\{A_k\right\}_{1 \leq k<\infty}$ is a sequence of pairwise disjoint events then
$$
P\left(\bigcup_{k=1}^{\infty} A_k\right)=\sum_{k=1}^{\infty} P\left(A_k\right) .
$$
\end{definition}

\begin{note}
    In mathematical analysis and in probability theory, a $\boldsymbol{\sigma}$-algebra (also $\boldsymbol{\sigma}$-field) on a set $X$ is a nonempty collection $\Sigma$ of subsets of $X$ closed under complement, countable unions, and countable intersections. The ordered pair $(X, \Sigma)$ is called a measurable space.
\end{note}


\section{Inclusion Exclusion Principle}

\begin{theorem}[inclusion-exclusion principle]
Let $A_1, A_2, A_3, \ldots$ be events in some probability space $(\Omega, \mathcal{F}, P)$. Then for each integer $n \geq 2$,
$$
\begin{aligned}
P\left(A_1 \cup \cdots \cup A_n\right)= & \sum_{i=1}^n P\left(A_i\right)-\sum_{1 \leq i_1<i_2 \leq n} P\left(A_{i_1} \cap A_{i_2}\right) \\
& +\sum_{1 \leq i_1<i_2<i_3 \leq n} P\left(A_{i_1} \cap A_{i_2} \cap A_{i_3}\right) \\
& -\sum_{1 \leq i_1<i_2<i_3<i_4 \leq n} P\left(A_{i_1} \cap A_{i_2} \cap A_{i_3} \cap A_{i_4}\right) \\
& +\cdots+(-1)^{n+1} P\left(A_1 \cap \cdots \cap A_n\right) . \\
= & \sum_{k=1}^n(-1)^{k+1} \sum_{1 \leq i_1<\cdots<i_k \leq n} P\left(A_{i_1} \cap \cdots \cap A_{i_k}\right) .
\end{aligned}
$$
This is called the inclusion-exclusion identity.
\end{theorem}

\section{Monotonicity and Countable Subadditivity}

\begin{proposition}
Let $A, B, A_1, A_2, A_3, \ldots$ be events in some probability space $(\Omega, \mathcal{F}, P)$\\
(i) Monotonicity: if $A \subset B$ then $P(A) \leq P(B)$.\\
(ii) Countable subadditivity: for any sequence of events $\left\{A_k\right\}$,
$$
P\left(\bigcup_{k=1}^{\infty} A_k\right) \leq \sum_{k=1}^{\infty} P\left(A_k\right) .
$$
Countable subadditivity generalizes the countable additivity axiom in a natural way. Its truth should be fairly obvious because the union $\bigcup_{k=1}^{\infty} A_k$ can have overlaps whose probabilities are then counted several times over in the sum $\sum_{i=1}^{\infty} P\left(A_k\right)$. By taking $A_k=\varnothing$ for all $k>n$ we get a finite version of subadditivity:
$$
P\left(A_1 \cup \cdots \cup A_n\right) \leq P\left(A_1\right)+\cdots+P\left(A_n\right)
$$
valid for all events $A_1, \ldots, A_n$.
\end{proposition}

\begin{corollary}
Let $\left\{A_k\right\}$ be a sequence of events on $(\Omega, \mathcal{F}, P)$.\\
\; (i) If $P\left(A_k\right)=0$ for all $k$, then $P\left(\bigcup_k A_k\right)=0$.\\
\; (ii) If $P\left(A_k\right)=1$ for all $k$, then $P\left(\bigcap_k A_k\right)=1$.
\end{corollary}

\section{Continuity of Probability}
\begin{definition}
    Suppose $\left\{A_k\right\}_{k \in \mathbb{Z}_{>0}},\left\{B_k\right\}_{k \in \mathbb{Z}_{>0}}, A$, and $B$ are events in a probability space
    $(\Omega, \mathcal{F}, P)$. We say that $A_k$ increases up to $A$ and use the notation
$$
A_k \nearrow A
$$
if the events $A_k$ are nested nondecreasing, which means that $A_1 \subset A_2 \subset A_3 \subset$ $\cdots \subset A_k \subset \cdots$, and $A=\bigcup_k A_k$. Figure 1 illustrates.
Analogously, we say that $B_k$ decreases down to $B$ and use the notation
$$
B_k \searrow B
$$
if the events $B_k$ are nested nonincreasing, which means that $B_1 \supset B_2 \supset B_3 \supset \cdots \supset$ $B_k \supset \cdots$, and $B=\bigcap_k B_k$
\end{definition}
\begin{theorem}

If $A_k \nearrow A$ or $A_k \searrow A$, then the probabilities converge: $\lim _{k \rightarrow \infty} P\left(A_k\right)=$ $P(A)$
\end{theorem}

\section{Conditional Probability}
\begin{definition}{conditional probability}
Let $B$ be an event on the probability space $(\Omega, \mathcal{F}, P)$ such that $P(B)>0$. Then for all events $A \in \mathcal{F}$ the conditional probability of $A$ given $B$ is defined as
$$
P(A \mid B)=\frac{P(A B)}{P(B)}
$$
\end{definition}

\begin{proposition}
  Let $B$ be an event on the probability space $(\Omega, \mathcal{F}, P)$ such that $P(B)>0$. Then as a function of the event $A, P(A \mid B)$ is a probability measure on $(\Omega, \mathcal{F})$
\end{proposition}

\begin{theorem}
In each statement below all events are on the same probability $\operatorname{space}(\Omega, \mathcal{F}, P)$\\
(a) Let $A$ and $B$ be two events and assume $P(B)>0$. Then
$$
P(A B)=P(B) P(A \mid B)
$$
Let $A_1, \ldots, A_n$ be events and assume $P\left(A_1 \cdots A_{n-1}\right)>0$. Then
$$
P\left(A_1 A_2 \cdots A_n\right)=P\left(A_1\right) P\left(A_2 \mid A_1\right) P\left(A_3 \mid A_1 A_2\right) \cdots P\left(A_n \mid A_1 \cdots A_{n-1}\right)
$$
(b) Let $\left\{B_i\right\}$ be a countable partition of $\Omega$ and $A$ any event. Then
$$
P(A)=\sum_{i: P\left(B_i\right)>0} P\left(A \mid B_i\right) P\left(B_i\right) .
$$
The sum above ranges over those indices $i$ such that $P\left(B_i\right)>0$.
\end{theorem}

\section{Bayes' Formula}

\begin{theorem}{Bayes' formula}
Let $\left\{B_k\right\}$ be a countable partition of the sample space $\Omega$. Then for any event $A$ with $P(A)>0$ and each $k$ such that $P\left(B_k\right)>0$,
$$
P\left(B_k \mid A\right)=\frac{P\left(A B_k\right)}{P(A)}=\frac{P\left(A \mid B_k\right) P\left(B_k\right)}{\sum_{i: P\left(B_i\right)>0} P\left(A \mid B_i\right) P\left(B_i\right)}
$$
\end{theorem}

\section{Independent Events}

\begin{definition}
    Two events $A$ and $B$ are independent if
$$
P(A B)=P(A) P(B)
$$
\end{definition}

\begin{theorem}
Suppose that $A$ and $B$ are independent events. Then the same is true for each of these pairs: $A^c$ and $B, A$ and $B^c$, and $A^c$ and $B^c$.
\end{theorem}

The definition of independence of more than two events requires that the product property hold for any subcollection of events.

\begin{definition}
(a) Events $A_1, \ldots, A_n$ are independent (or mutually independent) if for every collection $A_{i_1}, \ldots, A_{i_k}$, where $2 \leq k \leq n$ and $1 \leq i_1<i_2<\cdots<$ $i_k \leq n$
$$
P\left(A_{i_1} A_{i_2} \cdots A_{i_k}\right)=P\left(A_{i_1}\right) P\left(A_{i_2}\right) \cdots P\left(A_{i_k}\right)
$$\\
(b) Let $\left\{A_k\right\}_{k \in \mathbb{Z}_{>0}}$ be an infinite sequence of events in a probability space $(\Omega, \mathcal{F}, P)$. Then events $\left\{A_k\right\}_{k \in \mathbb{Z}_{>0}}$ are independent if for each $n \in \mathbb{Z}_{>0}$, events $A_1, \ldots, A_n$ are independent.
\end{definition}

\begin{theorem}
(a) Suppose events $A_1, \ldots, A_n$ are mutually independent. Then for every collection $A_{i_1}, \ldots, A_{i_k}$, where $2 \leq k \leq n$ and $1 \leq i_1<i_2<\cdots<i_k \leq n$, we have
$$
P\left(A_{i_1}^* A_{i_2}^* \cdots A_{i_k}^*\right)=P\left(A_{i_1}^*\right) P\left(A_{i_2}^*\right) \cdots P\left(A_{i_k}^*\right)
$$
where each $A_i^*$ can represent either $A_i$ or $A_i^c$.\\
(b) Let $\left\{A_k\right\}_{k \geq 1}$ be a finite or infinite sequence of independent events. Let $0=k_0<k_1<\cdots<k_n$ be integers. Let $B_1, \ldots, B_n$ be events constructed from the $A_k s$ so that, for each $j=1, \ldots, n, B_j$ is formed by applying set operations to $A_{k_{j-1}+1}, \ldots, A_{k_j}$. Then the events $B_1, \ldots, B_n$ are independent.
\end{theorem}

\begin{definition}
Let $A_1, \ldots, A_n$ and $B$ be events on $(\Omega, \mathcal{F}, P)$ and assume $P(B)>$ 0 . Then events $A_1, \ldots, A_n$ are conditionally independent, given $B$, if for every collection $A_{i_1}, \ldots, A_{i_k}$, where $2 \leq k \leq n$ and $1 \leq i_1<i_2<\cdots<i_k \leq n$,
$$
P\left(A_{i_1} A_{i_2} \cdots A_{i_k} \mid B\right)=\prod_{j=1}^k P\left(A_{i_j} \mid B\right)
$$
\end{definition}

\chapter{Random Variables and Probability Distributions}
\section{Random Variables}
Often we are interested in some numerical value associated to the outcome of
a random experiment. This just means that we are interested in the value of a
function that maps the elements of the sample space into the real numbers. These
functions are called random variables.

\begin{definition}
Let $(\Omega, \mathcal{F}, P)$ be a probability space. A random variable on $\Omega$ is a real valued function $X: \Omega \rightarrow \mathbb{R}$, for which $\{\omega \in \Omega: X(\omega) \leq c\} \in \mathcal{F}$ for any $c \in \mathbb{R}$.
\end{definition}

There is a simple way to encode an events as a random variable.

\begin{definition}
 Let $B$ be an event in a probability space. Then the indicator function (or indicator random variable) of $B$ is defined as the function
$$
I_B(\omega)= \begin{cases}1, & \text { if } \omega \in B \\ 0, & \text { if } \omega \notin B\end{cases}
$$
\end{definition}

\begin{note}
Note that $I_B$ is a random variable.
\end{note}
\section{Probability Distributions}

Through the probabilities of events of type $\{X \in B\}$, a random variable induces a probability measure on the real line.

\begin{definition}
Let $X$ be a random variable defined on the probability space $(\Omega, \mathcal{F}, P)$. The probability distribution of $X$ is the probability measure $\mu$ on $\mathbb{R}$ defined by
$$
\mu(B)=P(X \in B)
$$
for Borel subsets $B$ of $\mathbb{R}$.
\end{definition}

\begin{note}
    In mathematics, a Borel set is any set in a topological space that can be formed from open sets (or, equivalently, from closed sets) through the operations of countable union, countable intersection, and relative complement. Borel sets are named after √âmile Borel.
\end{note}


\end{document}